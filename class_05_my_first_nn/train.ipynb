{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from heart_desease_dataset import HeartDeseaseDataset\n",
    "\n",
    "from mp_model import MPModel\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, it 56, loss: 0.26199495792388916\n",
      "Epoch: 1, it 56, loss: 0.4517425000667572\n",
      "Epoch: 2, it 56, loss: 0.4124743938446045\n",
      "Epoch: 3, it 56, loss: 0.3021504580974579\n",
      "Epoch: 4, it 56, loss: 0.2033325433731079\n",
      "Epoch: 5, it 56, loss: 0.22402368485927582\n",
      "Epoch: 6, it 56, loss: 0.2531684637069702\n",
      "Epoch: 7, it 56, loss: 0.05162167549133301\n",
      "Epoch: 8, it 56, loss: 0.06957464665174484\n",
      "Epoch: 9, it 56, loss: 0.3176189959049225\n",
      "Epoch: 10, it 56, loss: 0.23719382286071777\n",
      "Epoch: 11, it 56, loss: 0.0141450809314847\n",
      "Epoch: 12, it 56, loss: 0.04735295847058296\n",
      "Epoch: 13, it 56, loss: 0.07966816425323486\n",
      "Epoch: 14, it 56, loss: 0.04112572968006134\n",
      "Epoch: 15, it 56, loss: 0.006863874848932028\n",
      "Epoch: 16, it 56, loss: 0.008787834085524082\n",
      "Epoch: 17, it 56, loss: 0.15911220014095306\n",
      "Epoch: 18, it 56, loss: 0.06267353147268295\n",
      "Epoch: 19, it 56, loss: 8.841188900987618e-06\n",
      "Epoch: 20, it 56, loss: 0.14271168410778046\n",
      "Epoch: 21, it 56, loss: 0.025151953101158142\n",
      "Epoch: 22, it 56, loss: 0.04091561958193779\n",
      "Epoch: 23, it 56, loss: 0.018916741013526917\n",
      "Epoch: 24, it 56, loss: 0.004189157392829657\n",
      "Epoch: 25, it 56, loss: 0.054554667323827744\n",
      "Epoch: 26, it 56, loss: 0.3556123673915863\n",
      "Epoch: 27, it 56, loss: 0.08908307552337646\n",
      "Epoch: 28, it 56, loss: 0.0007643983699381351\n",
      "Epoch: 29, it 56, loss: 0.31151899695396423\n",
      "Epoch: 30, it 56, loss: 0.013648794032633305\n",
      "Epoch: 31, it 56, loss: 0.006891132798045874\n",
      "Epoch: 32, it 56, loss: 4.371003399228357e-07\n",
      "Epoch: 33, it 56, loss: 0.24507683515548706\n",
      "Epoch: 34, it 56, loss: 0.22609688341617584\n",
      "Epoch: 35, it 56, loss: 0.003277750452980399\n",
      "Epoch: 36, it 56, loss: 0.025686435401439667\n",
      "Epoch: 37, it 56, loss: 0.09404953569173813\n",
      "Epoch: 38, it 56, loss: 0.12243342399597168\n",
      "Epoch: 39, it 56, loss: 0.0017768674297258258\n",
      "Epoch: 40, it 56, loss: 0.0862250030040741\n",
      "Epoch: 41, it 56, loss: 0.11752168089151382\n",
      "Epoch: 42, it 56, loss: 0.10356501489877701\n",
      "Epoch: 43, it 56, loss: 0.0030703076627105474\n",
      "Epoch: 44, it 56, loss: 0.08417921513319016\n",
      "Epoch: 45, it 56, loss: 5.960463411724959e-08\n",
      "Epoch: 46, it 56, loss: 0.08587253838777542\n",
      "Epoch: 47, it 56, loss: 0.249712273478508\n",
      "Epoch: 48, it 56, loss: 7.337859278777614e-05\n",
      "Epoch: 49, it 56, loss: 0.09300688654184341\n",
      "Epoch: 50, it 56, loss: 0.2799017131328583\n",
      "Epoch: 51, it 56, loss: 0.0022719204425811768\n",
      "Epoch: 52, it 56, loss: 0.00031455655698664486\n",
      "Epoch: 53, it 56, loss: 3.0000835522514535e-06\n",
      "Epoch: 54, it 56, loss: 0.0003005703038070351\n",
      "Epoch: 55, it 56, loss: 0.00019786164921242744\n",
      "Epoch: 56, it 56, loss: 0.0034652119502425194\n",
      "Epoch: 57, it 56, loss: 0.0011947150342166424\n",
      "Epoch: 58, it 56, loss: 0.4888538122177124\n",
      "Epoch: 59, it 56, loss: 0.1087794303894043\n",
      "Epoch: 60, it 56, loss: 0.00014681117318104953\n",
      "Epoch: 61, it 56, loss: 0.10219389200210571\n",
      "Epoch: 62, it 56, loss: 7.425139483530074e-05\n",
      "Epoch: 63, it 56, loss: 0.04912136122584343\n",
      "Epoch: 64, it 56, loss: 0.0015000988496467471\n",
      "Epoch: 65, it 56, loss: 0.004160899668931961\n",
      "Epoch: 66, it 56, loss: 4.1722983041836414e-06\n",
      "Epoch: 67, it 56, loss: 0.10663076490163803\n",
      "Epoch: 68, it 56, loss: 0.0008252367842942476\n",
      "Epoch: 69, it 56, loss: 0.00041924937977455556\n",
      "Epoch: 70, it 56, loss: 0.25908854603767395\n",
      "Epoch: 71, it 56, loss: 1.3907744289554103e-07\n",
      "Epoch: 72, it 56, loss: 0.011949330568313599\n",
      "Epoch: 73, it 56, loss: 0.0020701477769762278\n",
      "Epoch: 74, it 56, loss: 0.0\n",
      "Epoch: 75, it 56, loss: 0.02458302490413189\n",
      "Epoch: 76, it 56, loss: 0.007284694816917181\n",
      "Epoch: 77, it 56, loss: 9.417368346475996e-06\n",
      "Epoch: 78, it 56, loss: 0.06779922544956207\n",
      "Epoch: 79, it 56, loss: 0.0018784938147291541\n",
      "Epoch: 80, it 56, loss: 0.06744293123483658\n",
      "Epoch: 81, it 56, loss: 0.0002255221043014899\n",
      "Epoch: 82, it 56, loss: 0.0027092432137578726\n",
      "Epoch: 83, it 56, loss: 4.351082225184655e-06\n",
      "Epoch: 84, it 56, loss: 3.973642392907095e-08\n",
      "Epoch: 85, it 56, loss: 0.7356567978858948\n",
      "Epoch: 86, it 56, loss: 0.15146519243717194\n",
      "Epoch: 87, it 56, loss: 5.563007562159328e-06\n",
      "Epoch: 88, it 56, loss: 0.5494703650474548\n",
      "Epoch: 89, it 56, loss: 0.0005904099089093506\n",
      "Epoch: 90, it 56, loss: 0.06788168102502823\n",
      "Epoch: 91, it 56, loss: 0.005922377109527588\n",
      "Epoch: 92, it 56, loss: 5.9604641222676946e-08\n",
      "Epoch: 93, it 56, loss: 1.827865730774647e-06\n",
      "Epoch: 94, it 56, loss: 0.015352980233728886\n",
      "Epoch: 95, it 56, loss: 0.0\n",
      "Epoch: 96, it 56, loss: 0.0\n",
      "Epoch: 97, it 56, loss: 2.523244120311574e-06\n",
      "Epoch: 98, it 56, loss: 0.0\n",
      "Epoch: 99, it 56, loss: 0.09229082614183426\n"
     ]
    }
   ],
   "source": [
    "dataset_train = HeartDeseaseDataset('dataset', train=True)\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset_train, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "device = 'mps'\n",
    "\n",
    "model = MPModel(in_size=30, n_classes=2)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) \n",
    "\n",
    "total_epochs = 100\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    # it_pbar = tqdm(enumerate(train_data_loader), desc='')\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        x, y = batch\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch}, it {idx}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m         y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     corret_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m y_hat\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39meq(y)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal Acurancy on test set is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorret_samples\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_dataset)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m )\n",
      "File \u001b[0;32m~/projects/pytorch_mentorship/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/pytorch_mentorship/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/pytorch_mentorship/class_05_my_first_nn/mp_model.py:20\u001b[0m, in \u001b[0;36mMPModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/pytorch_mentorship/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/pytorch_mentorship/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/pytorch_mentorship/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/projects/pytorch_mentorship/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/pytorch_mentorship/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/pytorch_mentorship/.venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "test_dataset    = HeartDeseaseDataset('dataset', train=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "corret_samples = 0\n",
    "for x_batch, y_batch in tqdm(test_dataloader):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    x, y = batch\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(x)\n",
    "\n",
    "    corret_samples += y_hat.argmax(dim=1).eq(y).sum()\n",
    "\n",
    "print(f'Final Acurancy on test set is: {corret_samples/len(test_dataset)*100}%' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
